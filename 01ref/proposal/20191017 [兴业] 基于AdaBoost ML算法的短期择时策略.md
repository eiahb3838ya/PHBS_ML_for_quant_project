1.资料来源

20191017 [兴业] 基于AdaBoost ML算法的短期择时策略

2.参考价值

用因子来预测wind全A的未来涨跌。+1和-1.

短期择时的困难：1.情绪影响 **2.筛选有效因子 3.非线性因子如何建模 4.因子相关性如何解决 5.因子较多时如何避免过拟合**

ML在这里的意义是：**解决了2. 3. 4. 5.** 

51种日频因子数据构建基于决策树的AdaBoost分类起，从而对下一天的涨跌 +-1做预测。

51种因子的数据比较丰富，包括回购利率，信用利差，南华商品指数收益率，金油比，SP500index等多市场信息。

LS策略20141027-20190830获得了41.31%的收益率和1.41的sp ratio，L策略24.57%和0.98的sp ratio。如果是同期的holding策略是7.66%和0.26.

- Model 1

sklearn的决策树里用基尼系数和cross entropy来做为决策树的选项。正文的回测时间是20070601-20190830，扩展窗口（expanding）：

1. 1800天的样本数据training，利用训练集进行kFold（k=5）选择准确率最高的model，要搜索hyperparameter（决策度不纯度：信息熵，Gini系数；决策树深度：5，10，...，30）
2. 利用train的model来预测 wind全A指数下一交易日的涨跌，涨就在当日收盘买入wind全A，跌就卖wind全A。
3. 每多20天重新train一次model，并把这个数据扔进步骤1，重新1和2循环。

【很奇怪的是它的图5，简单holding的收益比多空和多组合都要好...这是为什么呢？这是因为出现了过拟合，样本内好，但样本外很差，很合理吧，这个报告是用python写的code】

- Model2

为了解决这个问题，基于**多个单层决策树**构建择时模型，就开始了Boost Model。通过等权叠加多个单层决策树构建择时模型。

思路是：对51个因子构建**单层决策树**，筛选出能带来超额收益的factor和model，根据model来预测涨跌。最后多个单层决策树叠加。【这一步就在筛选有效因子，其实思想是挺合理的，但也没有尝试因子合成这样的事情】

1. 1800天作为train，对51个因子分别做单层决策树
2. 计算51个单层决策树做出来对择时收益率序列，筛选5%显著性水平的决策树。
3. 对这些筛选出来的决策树 预测 wind全A下一日的涨跌，对决策树的结果取mean。
4. 连续使用2中的20个交易日进入model1，加入train。

【换仓信号发出当日收盘 无交易成本，为什么前面设置了交易成本这里却是0呢？因为思路是先设置一个0交易费的模型来看因子是不是真的好，如果真的好再考虑交易费的问题】

结论是 多个单层的决策树 比 单个多层的决策树 要好。Boost以后样本外确实变好了。

- Model3

然后又觉得 单层决策树难以解决 因子和收益的非线性问题，有效因子基本上同时间51个里只有1个有效。用Adaboost，理论上不需要要做特征工程的筛选，模型也不容易过拟合。Adaptive Boosting自适应体现在当前的弱分类器的分类错误的样本权重会增大，从而在训练下一个基分类器时会拟合之前分类错的样本。

AdaBoost会选择51个因子里的其中一个作为决策树的节点来对wind全A下一日的涨跌做预测。下一日的最终预测结果由所有基分类器共同决定。

开始回测了一个没有交易成本，之后加进去在换仓信号发出次日开盘时进入新的仓位【？？？这是什么，是开盘换仓的意思吗】以及单边万分之五的交易成本。

年化22%以上，sp ratio = 0.9.

------

后半部分开始。

**水晶球模型A**（利用期权市场的信息，上证50ETF，之前的某篇研报里的有用策略），加上前面的**Adaboost模型B**，由于两个策略关联度低0.02，形成了**双塔奇兵择时模型C**【这个名字真魔幻】。C=A+B

A好，B好，所以C应该更好。

单边万5的交易成本【这个合理吧，如果买是万3，卖是万3的话】，LS策略20150601-20190830收益率15.90%和sp ratio 0.72。同期如果不做择时的wind全A -11.18%。【其实我觉得这个对标的东西不对，这个baseline的model应该是logistic regression这样的回归作出的择时策略，而不是简单的holding】

3.复现价值

学习到了可以先设置0交易费，再真的有效的时候加入交易费率。

Adaboost 作为boost策略的一种可以提升效果，gbdt和xgboost其实也可以试一下呢。

信息增益率可以作为一种筛选有效因子的方法。

4.会遇到的困难

我觉得这篇像实习生交的大作业...

最大的困难应该是clean data的困难，和 超参数的选择，但python可以grid网格搜索超参数，所以可能困难在于训练每个基分类器的效果要好。